{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "MODEL_DEBUG = False\n",
    "MODEL_IMPROVE_DEBUG = False\n",
    "GPU_COUNT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://static.vecteezy.com/system/resources/previews/000/108/974/original/vector-fashion-girl.jpg\" width=\"700px\" />\n",
    "\n",
    "## Fashion-MNIST Classification\n",
    "\n",
    "> Can we develop a model that performs well on the Fashion-MNIST dataset?\n",
    "\n",
    "### Context\n",
    "Fashion-MNIST is a dataset from Zalando research, comprising of <code>28 x 28</code> grayscale images of a total fo <code>70000</code> fashion products from 10 categories. The intent of this dataset is to serve as a replacement for the MNIST dataset for testing and benchmarking machine learning and deep learning models (Xiao, Rasul and Vollgraf, 2017).\n",
    "\n",
    "The dataset is based on images from Zalando's assortment. Each original image of <code>762 x 1000</code> in JPEG format was converted to PNG, resized, and then converted to grayscale.\n",
    "\n",
    "### Objectives\n",
    "<ol>\n",
    "\t<li>To explore and understand the dataset</li>\n",
    "\t<li>Understand the effects of different data augmentation techniques on the performance of the model</li>\n",
    "\t<li>Develop and experiment with models in order to rival state-of-the-art (SOTA) benchmark scores</li>\n",
    "</ol>\n",
    "\n",
    "## Importing libraries\n",
    "We import the necessary libraries for the notebook to run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q einops\n",
    "!pip install -q pandas\n",
    "!pip install -q seaborn\n",
    "!pip install -q matplotlib\n",
    "!pip install -q opencv-python\n",
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# if not DEBUG:\n",
    "# \tfrom MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set('notebook')\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import os\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the seed such that the notebook results in reproducible results when run. \n",
    "\n",
    "We also set the device to CUDA to enable torch to use our GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available now:', device)\n",
    "if device != torch.device('cuda'):\n",
    "    print('using cpu, exiting')\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "Below we define some utility functions that will ease and help us with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_data(data, loc):\n",
    "\tdatacopy = copy.deepcopy(data)\n",
    "\tarr = np.array(datacopy.loc[loc].drop('label'))\n",
    "\tlabel = datacopy.loc[loc]['label']\n",
    "\troot = int(len(arr) ** 0.5)\n",
    "\tarr.resize((root, root))\n",
    "\treturn label, arr\n",
    "\n",
    "def imshow(arr: list, label: list = None, grayscale=True, figsize=None):\n",
    "\tif label == None:\n",
    "\t\tlabel = [''] * len(arr)\n",
    "\n",
    "\theight = int(len(arr) ** 0.5)\n",
    "\twidth = math.ceil(len(arr) / height)\n",
    "\n",
    "\tif figsize == None:\n",
    "\t\tfig = plt.figure()\n",
    "\telse:\n",
    "\t\tfig = plt.figure(figsize=figsize)\n",
    "\tfor i in range(height):\n",
    "\t\tfor j in range(width):\n",
    "\t\t\tax = fig.add_subplot(height, width, i * height + j + 1)\n",
    "\t\t\tax.grid(False)\n",
    "\t\t\tax.set_xticks([])\n",
    "\t\t\tax.set_yticks([])\n",
    "\t\t\tax.imshow(arr[i * height + j], cmap='gray' if grayscale else '')\n",
    "\t\t\tax.set_title(label[i * height + j])\n",
    "\n",
    "def df_to_tensor(df, shape = (28, 28)):\n",
    "\treturn torch.tensor(df.values.reshape((-1, *shape)), dtype=torch.float32)\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "\treturn df.applymap(lambda x: x / 255)\n",
    "\n",
    "def mse(t1, t2, shape=(28, 28)):\n",
    "\tloss = nn.MSELoss(reduction='none')\n",
    "\tloss_result = torch.sum(loss(t1, t2), dim=2)\n",
    "\tloss_result = torch.sum(loss_result, dim=2)\n",
    "\tloss_result = loss_result / np.prod([*shape])\n",
    "\treturn loss_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Let's take a look at the dataset. This dataset was retrieved from the original Fashion MNIST dataset found at <a href=\"https://arxiv.org/pdf/1708.07747v2.pdf\">Paper Link</a>. \n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "\t\t<th>\n",
    "\t\t\tColumn Name\n",
    "\t\t</th>\n",
    "\t\t<th>\n",
    "\t\t\tDescription\n",
    "\t\t</th>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>\n",
    "\t\t\tlabel\n",
    "\t\t</td>\n",
    "\t\t<td>\n",
    "\t\t\tThe true class of the image, represented as an integer ranging from 0 to 9<strong>*</strong>\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>\n",
    "\t\t\tpixel 1<br/>...<br/>pixel 784\n",
    "\t\t</td>\n",
    "\t\t<td>\n",
    "\t\t\tPixels representing the image, each pixel ranging from 0 to 255\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "<strong>\\*</strong>Each number represents a certain dress item\n",
    "```\n",
    "0 -> T-shirt/top\n",
    "1 -> Trouser\n",
    "2 -> Pullover\n",
    "3 -> Dress\n",
    "4 -> Coat\n",
    "5 -> Sandal\n",
    "6 -> Shirt\n",
    "7 -> Sneaker\n",
    "8 -> Bag\n",
    "9 -> Ankle boot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/fashion-mnist_train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are a total of <code>60000</code> rows and <code>785</code> columns, which is exactly what the original paper mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for missing values and invalid data\n",
    "Let's try to identify if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears <strong>no missing values are found</strong>. We will then check for any potential typos.\n",
    "<ol>\n",
    "\t<li>Firstly, the label should only consist of values from 0 - 9</li>\n",
    "\t<li>Secondly, the pixel values should only fall in between and inclusive of 0 - 255</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum and Maximum values of labels:', train['label'].min(), \"and\", train['label'].max())\n",
    "print(\"Number of values falling outside the range (0 - 255):\",\n",
    "\ttrain.applymap(\n",
    "\t\tlambda x: x < 0 or x > 255 # True if out of range\n",
    "\t).sum().sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the dataset seems to be properly cleaned, thus can proceed knowing that <strong>no sign of invalid or missing data have been found.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We'll convert the data such that the range <code>0 - 255</code> becomes <code>0 - 1</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.columns[train.columns != 'label']] = preprocess(train[train.columns[train.columns != 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "In the EDA, we'll be attempting to complete the following set of objectives:\n",
    "\n",
    "### EDA Objectives\n",
    "<ol>\n",
    "\t<li>\n",
    "\t\tIs there any <strong>class imbalance</strong>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tWhat does the <strong>average image</strong> look like?\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tAre there <strong>any outliers/anomalies</strong>?\n",
    "\t</li>\n",
    "</ol>\n",
    "Before we begin the EDA, we first need to remove a chunk of the training data to be used as validation dataset in order to avoid data leakage due to the decisions we make during EDA.\n",
    "\n",
    "### Validation Dataset\n",
    "Let's make our validation dataset. I decided that I will <strong>not be using cross validation during the modelling process</strong> due to the following reasons:\n",
    "<ul>\n",
    "\t<li>Time taken to train models, especially if utilising <strong>computationally expensive layers, activation functions or architecture</strong></li>\n",
    "\t<li>Additionally due to the sheer size of the dataset, not only does this add to increased training time but also means there is enough data such that cross validation is not necessary</li>\n",
    "</ul>\n",
    "\n",
    "We'll use a validation size of <code>10000</code> to mimic the size of the test dataset. We'll also stratify on the <code>label</code> to ensure that the model we generalizes best across all \n",
    "the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('label', axis=1), train['label'], test_size = 10000 / len(train), stratify=train['label'])\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data have been split correctly and now we can begin our EDA!\n",
    "\n",
    "### Signs of class imbalance?\n",
    "We'll first identify the situation with class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the distribution of labels to be similar for all classes, reaching roughly a count of `5000`. Thus, there is no sign of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the average image look like?\n",
    "Let's take a look at a random sample of image each first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "titles = []\n",
    "\n",
    "for i in range(10):\n",
    "\tidx = random.sample(list(y_train[y_train == i].index), 1)\n",
    "\timages.append(X_train.loc[idx].values.reshape((28, 28)))\n",
    "\ttitles.append(classes[y_train.loc[idx].values[0]])\n",
    "\n",
    "imshow(images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that sometimes, there appears to be certain images that are <strong>darker</strong> than others. This suggests that no form of normalization or scaling has been applied to the dataset. Let's take a look at the distribution of brightness to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "\n",
    "# Note: we want to IGNORE the background of black, filtering by using x != 0\n",
    "sns.histplot(X_train.apply(lambda x: x[x != 0].mean(), axis=1).values, ax=ax) \n",
    "ax.set_xlabel('Average Brightness per Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ol>\n",
    "\t<li>\n",
    "\t\tMost of the images are on the brighter side as we see a large bulk of the distribution going towards the right side.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tThe distribution is skewed to the left, which suggests that we have a few images which are quite dark.\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To find out what the average image looks like, we'll perform an average of the pixels across the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([\n",
    "\tX_train.mean().apply(lambda x: x).values.reshape(28, 28)\n",
    "], ['Average of all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ol>\n",
    "\t<li>The corners of the images are dark/black. This means there is likely no piece of clothing the spreads across the image.</li>\n",
    "\t<li>There is a bright circle-shape in the middle. This tells us most of these images are centered around in the middle</li>\n",
    "\t<li>An <strong>interesting note to point out</strong> is there seems to be some sort of darker 'bar' in the middle, separating the lightest parts of the image.</li>\n",
    "\t<li>\n",
    "\t\tThis may be due to the presence of trousers in the dataset, as from previous images, we can see trousers appears to be the only apparel with such a contrasting stripe in the middle.\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average image among the classes\n",
    "Next we'll split by the classes and find the average among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "titles = []\n",
    "\n",
    "for i in range(10):\n",
    "\tidx = random.sample(list(y_train[y_train == i].index), 1)\n",
    "\timages.append(X_train[y_train == i].mean().apply(lambda x: x).values.reshape(28, 28))\n",
    "\ttitles.append(f'Avg {classes[y_train.loc[idx].values[0]]}')\n",
    "\n",
    "imshow(images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ol>\n",
    "\t<li>Each type of apparel generally has the same shape</li>\n",
    "\t<li>The most <strong>spread out/least consistent</strong> class seems to be sandals, as seen from the opaque and thicker edges, where as the other types have sharper and lighter borders.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any outliers/anomalies?\n",
    "\n",
    "We see from the average images, that the piece of clothing in the image are <strong>centered and upright</strong>, in that there are no sort of rotations going on.\n",
    "\n",
    "As such, let's see if there may be any outlier images, such as rotations or anomaly white spots on the image.\n",
    "\n",
    "We'll start by visualizing the dataset and seeing if outliers may exist. We can do this using <code>tSNE</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG and False:\n",
    "\ttsne = TSNE(n_jobs=-1)\n",
    "\tTSNE()\n",
    "\tdata_2d = tsne.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG and False:\n",
    "\tvis_x = data_2d[:, 0]\n",
    "\tvis_y = data_2d[:, 1]\n",
    "\tplt.scatter(vis_x, vis_y, c=y_train, cmap=plt.cm.get_cmap(\"jet\", 10), marker='.')\n",
    "\tcbar = plt.colorbar(ticks=range(10))\n",
    "\tcbar.set_ticks([])\n",
    "\tfor j, lab in enumerate(classes):\n",
    "\t\tcbar.ax.text(1.5, j, f'${lab}$', ha='left', va='center')\n",
    "\n",
    "\tcbar.ax.get_yaxis().labelpad = 15\n",
    "\n",
    "\tplt.clim(-0.5, 9.5)\n",
    "\tplt.title('Clustering Visualization with 2 components')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE produces an <strong>incredibly interesting visualization</strong>. We observe:\n",
    "<ol>\n",
    "\t<li>\n",
    "\t\tThere is a large breakoff of clusters at the top right. Taking a look at the classes we see that they are <code>Sneaker</code>, <code>Ankleboot</code> and <code>Sandal</code>, which are all types of footwear. \n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tWithin the footwear cluster, we recognise the classes are separated distinctly, with quite sharp edges. \n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tThe large cluster in the middle contains <code>pullover</code>, <code>dress</code>, <code>t-shirt</code> and <code>shirt</code>. This cluster seems to represent the upper body clothing.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tIn this center cluster, the classes do not have distinct edges, but <strong>rather seem to mix and mingle together</strong>. This suggests that <code>TSNE with 2 components was unable</code> to separate these classes.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tLastly, the cluster at the bottom left represents <code>bag</code>. We recognise a distinct split in the center of the cluster, suggesting that there are two types of bags.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tLooking at the top right cluster, we recognise some red and blue data points in the wrong cluster. This may suggest <strong>presence of outliers</strong>. Let's try to identify outliers and see how we can deal with them.\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Autoencoders to identify anomalies\n",
    "The idea behind this is that we can use an autoencoder to some what <strong>compress</strong> the image, or restrict the image to its most important features. We'll train this autoencoder on minimising MSE with the original image, thus the autoencoder will learn to capture the more generic and most important features. \n",
    "\n",
    "Thus, after an anomaly has been passed through this autoencoder, we expect the MSE to be quite high as the anomaly image does not follow the general 'style' of the images in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the AutoEncoder architecture\n",
    "We'll use a <strong>Convolutional Auto Encoder architecture</strong>, which is better able to extract features from images. This is because artificial neural networks do not account for proximity of each pixel to other pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(8, 16, 3, padding=1, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=0, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Flatten(start_dim = 1),\n",
    "            nn.Linear(32 * 3 * 3, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32 * 3 * 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Unflatten(1, torch.Size([32, 3, 3]))\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, output_padding=0, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, 3, padding=1, output_padding=1, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, 3, padding=1, output_padding=1, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up the architecture, we can instantiate the model, optimizers and criterion. To train the autoencoder, we'll be using <strong>mean squared error</strong> loss.\n",
    "\n",
    "$$MSE = \\frac{1}{n ^ 2}\\sum_{i = 0}^{n}\\sum_{j = 0}^{n}(X_{ij} - \\hat{X}_{ij}) ^ 2$$\n",
    "\n",
    "where $X_{ij}$ is the true value of the pixel at the $\\text{i'th}$ row and $\\text{j'th}$ column and   \n",
    "$\\hat{X}_{ij}$ is the reconstructed value of the pixel at the $\\text{i'th}$ row and $\\text{j'th}$ column\n",
    "\n",
    "This is because any pixel that <strong>stands out/is an anomaly</strong> will have a <strong>heavily influence</strong> on the loss <strong>due to the square in the loss function</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "\tmodel = ConvAutoencoder()\n",
    "\tmodel = model.to(device)\n",
    "\tmodel = nn.DataParallel(model)\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=1e-3 * GPU_COUNT ** 0.5)\n",
    "\tcriterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset\n",
    "Next we need to load the dataset. In order to do this, below is a custom class that acts as a wrapper <strong>to convert a pandas dataframe to PyTorch tensors</strong>, such that we are able to load the dataset into the <code>DataLoader</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_prob_dist(labels: torch.tensor, size = 10):\n",
    "\tarr = np.full((len(labels), size), 0)\n",
    "\tfor i, label in enumerate(labels):\n",
    "\t\tarr[i][int(label.item())] = 1\n",
    "\treturn torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "class TorchDataset():\n",
    "\tdef __init__(self, X, y, X_shape = (1, 28, 28), y_shape = (1, )):\n",
    "\t\tif (type(X) == torch.Tensor):\n",
    "\t\t\tself.x = X.to(device)\n",
    "\t\telse:\n",
    "\t\t\tself.x = df_to_tensor(X, X_shape).to(device)\n",
    "\n",
    "\t\tif (type(y) == torch.Tensor):\n",
    "\t\t\tself.y = y.to(device)\n",
    "\t\telse:\n",
    "\t\t\tself.y = df_to_tensor(y, y_shape)\n",
    "\t\t\tself.y = to_prob_dist(self.y).to(device)\n",
    "\n",
    "\t\t# Transform labels to probability distributions\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.y)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TorchDataset(X_train, y_train), batch_size=128 * GPU_COUNT, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Convolutional Auto Encoder\n",
    "We can now train the auto encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "if not DEBUG:\n",
    "    n_epochs = 40\n",
    "\n",
    "    track_loss = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        for data in train_loader:\n",
    "            # _ stands in for labels, here\n",
    "            # no need to flatten images\n",
    "            images, _ = data\n",
    "            images = images.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(images)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, images)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*images.size(0)\n",
    "                \n",
    "        # print avg training statistics \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "        ))\n",
    "        track_loss.append(train_loss)\n",
    "\n",
    "    track_loss_df = pd.DataFrame({'epoch': np.arange(1, len(track_loss) + 1), 'loss': track_loss})\n",
    "    sns.lineplot(track_loss_df, x = 'epoch', y = 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretting the results\n",
    "Let's take a look at how the autoencoder works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "\timage = df_to_tensor(X_train.iloc[0], (1, 28, 28))\n",
    "\timage = image.to(device)\n",
    "\tencoder_output = model.module.encoder(image)\n",
    "\tmodel_output = model(image)\n",
    "\timshow([image.cpu().numpy()[0, 0], encoder_output.cpu().detach().numpy()[0, 0], model_output.cpu().detach().numpy()[0, 0]], ['Original', 'Encoder', 'AutoEncoder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained and working autoencoder, let's use it to identify any anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "\timages = df_to_tensor(X_train, (1, 28, 28))\n",
    "\timages = images.to(device)\n",
    "\toutputs = model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the <code>99.95'th percentile</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "\terrors = mse(outputs, images).detach().cpu().numpy().flatten()\n",
    "\terror_boolean = errors > np.percentile(errors, 99.95)\n",
    "\n",
    "\timshow(X_train.iloc[error_boolean].values.reshape((-1, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ol>\n",
    "\t<li>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis: Footwear images only point to the left\n",
    "\n",
    "From a view glimpses of footwear classes, we begin to recognise that it seems that all the images point to the left direction: in that the 'pointy' tip of the footwear is at the left and the heel is at the right.\n",
    "\n",
    "Here, let's take a look at 36 randomly sampled footwear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_footwear = np.isin(y_train.values, [5, 7, 9])\n",
    "footwear_images = X_train[is_footwear]\n",
    "\n",
    "imshow(\n",
    "\tfootwear_images.sample(36).values.reshape((-1, 28, 28)),\n",
    "\tfigsize=(10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Slope Algorithm for Footwear Images\n",
    "\n",
    "To test the orientation of footwear (i.e. whether it's pointed to the left or right), we can use a slope detection algorithm.\n",
    "\n",
    "The idea for this algorithm is below: \n",
    "\n",
    "```\n",
    "[\n",
    "\t[0, 0, 0, 0, 0],\n",
    "\t[0, 0, 0, 1, 1],\n",
    "\t[0, 1, 1, 1, 1],\n",
    "\t[1, 1, 1, 1, 0],\n",
    "\t[0, 0, 0, 0, 0]\n",
    "]\n",
    "```\n",
    "\n",
    "Suppose the above <code>5x5</code> is an image of our shoe. For each column, we'll find the first position where there is a non-zero value.\n",
    "\n",
    "So for the first column it would here:\n",
    "\n",
    "```\n",
    "[\n",
    "\t[0, 0, 0, 0, 0],\n",
    "\t[0, 0, 0, 1, 1],\n",
    "\t[0, 1, 1, 1, 1],\n",
    "\t[*1*, 1, 1, 1, 0],\n",
    "\t[0, 0, 0, 0, 0]\n",
    "]\n",
    "```\n",
    "\n",
    "For second column it would here:\n",
    "\n",
    "```\n",
    "[\n",
    "\t[0, 0, 0, 0, 0],\n",
    "\t[0, 0, 0, 1, 1],\n",
    "\t[0, *1*, 1, 1, 1],\n",
    "\t[1, 1, 1, 1, 0],\n",
    "\t[0, 0, 0, 0, 0]\n",
    "]\n",
    "```\n",
    "\n",
    "Doing this will give us something we can call a <strong>height</strong>. For the above image, our height would be:\n",
    "<code>[2, 3, 3, 4, 4]</code>\n",
    "\n",
    "From this, we can simply determine the direction in which the slant goes. If the slant goes from low to high, it is pointed to the left, otherwise, it is pointed to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_left_pointed(arr):\n",
    "\tarr = np.array(arr)\n",
    "\tgreater = np.where(arr.T > 0) # Gives us all index positions for non zero values\n",
    "\tgreater_df = pd.DataFrame(greater).T # A dataframe where the first column is the column position, and the second column is the row index for non-zero values\n",
    "\n",
    "\t# We will then group by the first column (column), finding the max height for each column\n",
    "\tY = len(arr) - greater_df.groupby(0).agg('min').values.flatten()\n",
    "\tX = np.arange(0, len(Y))\n",
    "\tslope = np.polyfit(X,Y,1)[0]\n",
    "\treturn slope > 0\n",
    "\n",
    "if not DEBUG:\n",
    "\tmylist = footwear_images.values.reshape((-1, 28, 28)).tolist()\n",
    "\tresult = np.array(list(map(is_left_pointed, mylist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "\timshow(footwear_images.values.reshape((-1, 28, 28))[result == False][0:49], figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ul>\n",
    "\t<li><strong>Most of these footwear appear to be slippers</strong>. This makes sense as our algorithm looks for the slope, which the slipper is different from something like a shoe.</li>\n",
    "\t<li>\n",
    "\t\t<strong>We also see images of footwear pointing to the right.</strong>\n",
    "\t</li>\n",
    "</ul>\n",
    "\n",
    "Thus we conclude that when performing data augmentation, we should remember to horizontally flip as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We have performed feature engineering during the <strong>preprocessing step</strong>, where we divided all the values of the pixels by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.1307, ), (0.3081, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is a technique used to increase the amount of data by adding slightly modified copies of existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model (Shorten and Khoshgoftaar, 2019).\n",
    "\n",
    "As such, performing data augmentation is likely to have a positive impact on our model performance, if our model overfits.\n",
    "\n",
    "### Basic\n",
    "For our basic augmentation, I decided to perform the following transformations:\n",
    "<ul>\n",
    "\t<li>\n",
    "\t\t<strong>RandomHorizontalFlip</strong> - this is due to the different directions the image would 'point' as seen from our EDA\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\t<strong>RandomRotation</strong> - The goal here is not to rotate by 90 degrees, but to make slight adjustments of angles such that the model becomes more robust.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\t<strong>RandomAffine</strong> - We'll be using affine to <strong>shear</strong> the images in the y-direction. \n",
    "\t</li>\n",
    "</ul>\n",
    "\n",
    "I realised there was no need to perform cropping augmentation, as working with the dataset has shown me that the images show the item fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flip = transforms.RandomHorizontalFlip(p=1)\n",
    "t_rotate = transforms.RandomRotation((-5, 5))\n",
    "\n",
    "\n",
    "t_shear = transforms.RandomChoice([\n",
    "\ttransforms.RandomAffine(0, shear = (-10, 10, -10, 10), interpolation=InterpolationMode.NEAREST)\n",
    "])\n",
    "\n",
    "t_basic = transforms.Compose([\n",
    "\ttransforms.RandomOrder([\n",
    "\t\tt_flip, t_rotate, t_shear\n",
    "\t])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see how each transformation contributes to the final image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = copy.deepcopy(df_to_tensor(X_train.iloc[2]))\n",
    "imshow([\n",
    "\tsample_img[0],\n",
    "\tt_flip(copy.deepcopy(sample_img))[0],\n",
    "\tt_rotate(copy.deepcopy(sample_img))[0],\n",
    "\t# t_shear(copy.deepcopy(sample_img))[0],\n",
    "\tt_basic(copy.deepcopy(sample_img))[0],\n",
    "], ['Original', 'Flipped', 'Rotate', 'Shear', 'Basic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomErasing\n",
    "An additionally method of augmentation we can try is <strong>RandomErasing</strong>. RandomErasing is a data augmentation technique which randomly selects a rectangular region of the image and erases those pixels with random values. (Zhong et al., 2017)\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_erase = transforms.RandomErasing(p = 1)\n",
    "\n",
    "imshow([\n",
    "\tsample_img[0],\n",
    "\tt_erase(copy.deepcopy(sample_img))[0],\n",
    "\tt_erase(copy.deepcopy(sample_img))[0],\n",
    "\tt_erase(copy.deepcopy(sample_img))[0],\n",
    "\tt_erase(copy.deepcopy(sample_img))[0],\n",
    "\tt_erase(copy.deepcopy(sample_img))[0],\n",
    "\tt_erase(copy.deepcopy(sample_img))[0],\n",
    "], ['Original', 'RandomErasing', 'RandomErasing', 'RandomErasing', 'RandomErasing', 'RandomErasing', 'RandomErasing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As random erasing is used on top of basic augmentation, we'll make a compose transformer of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_erase = transforms.Compose([\n",
    "\tt_basic,\n",
    "\ttransforms.RandomErasing(p = 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Metric\n",
    "We'll be using two metrics: <strong>Accuracy</strong> and <strong>Categorical Crossentropy</strong>\n",
    "\n",
    "$$accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "$$\\text{categorical crossentropy} = -\\sum_{i=1}^{n}y_i \\log{(p_i)} $$\n",
    "\n",
    "We use accuracy to choose the model, as the benchmarks for the Fashion MNIST dataset are in terms of accuracy. Additionally, the dataset does not have an imbalanced class issue, thus using accuracy is not unsuitable.\n",
    "\n",
    "We use categorical crossentropy as our criterion to train the model, because it heavily penalizes models that are confident and wrong in their prediction. This is due to the logarithmic function in the cross entropy formula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training environment\n",
    "Let's set up the necessary details for our training to begin. Firstly, <strong>we define our datasets below</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_augmentation(torch_dataset: TorchDataset, transform):\n",
    "\tdata_copy = copy.deepcopy(torch_dataset)\n",
    "\tX = copy.deepcopy(data_copy.x)\n",
    "\ttransformed = transform(X)\n",
    "\tdata_copy.x = torch.cat((X, transformed), dim = 0)\n",
    "\tdata_copy.y = torch.cat([data_copy.y, data_copy.y])\n",
    "\treturn data_copy\n",
    "\n",
    "train_data = TorchDataset(X_train, y_train)\n",
    "val_data = TorchDataset(X_val, y_val)\n",
    "train_augmented_erase = perform_augmentation(TorchDataset(X_train, y_train), t_erase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Early Stopping\n",
    "Additionally, instead of <strong>arbitrarily choosing the number of epochs</strong> or fine tuning it (which computationally will take long), I decided to use <strong>Early Stopping</strong> as a mechanism.\n",
    "\n",
    "Overfitting is something that occurs when a model begins to extract some of the residual variation such as noise under the assumption that it is learning the patterns (Burnham and Anderson, 2010). Essentially, it starts to <strong>memorize instead of generalize</strong>. Traditional Early Stopping attempts to combat overfitting by stopping training if the performance/loss of the model on a validation set does not improve over $n$ epochs, where $n$ is the patience parameter. However, it requires <strong>clever tuning</strong> to prevent underfitting (Adam, 2018). As such to address this problem, I use a custom implemented early stopping class inspired by this <a href=\"https://alexadam.ca/2018/08/03/early-stopping/\">article</a>.\n",
    "\n",
    "The idea of this custom early stopping is that we <strong>only stop training</strong> if there are no improvements in <strong>both loss and accuracy</strong>. If there is an improve in any of the single quantities, the patience counter is reset. This tackles the issue of the underlying assumption that <strong>\"loss is correlated with accuracy</strong>\", and becomes a more lenient form of early stopping, ideally decreasing underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEarlyStopping():\n",
    "\tdef __init__(self, patience, min_loss_delta = 0, min_acc_delta = 0):\n",
    "\t\tself.patience = patience\n",
    "\t\tself.best_loss = 1e9\n",
    "\t\tself.best_acc = 0\n",
    "\t\tself.patience_count = 0\n",
    "\t\tself.count = 0\n",
    "\t\tself.stop = False\n",
    "\n",
    "\t\tself.min_loss_delta = min_loss_delta\n",
    "\t\tself.min_acc_delta = min_acc_delta\n",
    "\n",
    "\tdef __call__(self, loss, accuracy):\n",
    "\t\tself.save_state = False\n",
    "\n",
    "\t\tif self.best_loss - loss > self.min_loss_delta or accuracy - self.best_acc > self.min_acc_delta:\n",
    "\t\t\tif self.best_loss - loss > self.min_loss_delta:\n",
    "\t\t\t\tself.best_loss = loss\n",
    "\t\t\tif accuracy - self.best_acc > self.min_acc_delta:\n",
    "\t\t\t\tself.best_acc = accuracy\n",
    "\n",
    "\t\t\tself.count = 0\n",
    "\t\telse:\n",
    "\t\t\tself.count += 1\n",
    "\t\t\n",
    "\t\tif self.count >= self.patience:\n",
    "\t\t\tself.stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "To help us with training and determining the best models, we set up a trainer that will <strong>keep track of our models</strong> and <strong>ease the modelling process</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accurate_count(pred, true):\n",
    "\treturn ((pred.argmax(dim=1) == true.argmax(dim=1)).sum()).item()\n",
    "\n",
    "\t\n",
    "class Trainer():\n",
    "\tdef __init__(self, criterion):\n",
    "\t\tself.saves = {}\n",
    "\t\tself.loss_tracker = {}\n",
    "\t\tself.history = pd.DataFrame()\n",
    "\n",
    "\tdef show_history(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef train_model(self, name, model_pass, optimizer_pass, optimizer_args,\n",
    "\t\t\t\t\ttrain_data: TorchDataset, scheduler_pass = None,\n",
    "\t\t\t\t\tscheduler_args = None, batch_size = GPU_COUNT * 128,\n",
    "\t\t\t\t\tearly_stopping = None, epochs = 200, verbose = False,\n",
    "\t\t\t\t\tval_data = val_data, save_model = False, augment = None,\n",
    "\t\t\t\t\treturn_model = False):\n",
    "\n",
    "\t\tmodel = model_pass()\n",
    "\t\tmodel = model.to(device)\n",
    "\t\tmodel = nn.DataParallel(model)\n",
    "        \n",
    "\t\tsm = nn.Softmax(dim=1)\n",
    "\n",
    "\t\toptimizer = optimizer_pass(model.parameters(), **optimizer_args)\n",
    "\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\ttrain_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "\t\tval_loader = DataLoader(val_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\t\tif scheduler_pass != None:\n",
    "\t\t\tscheduler = scheduler_pass(optimizer, **scheduler_args)\n",
    "\n",
    "\t\tdone = 0\n",
    "\t\tn_epochs = epochs\n",
    "\t\t\n",
    "\t\ttrack_loss = np.full((epochs, 4), 0.0, dtype=np.float32) # train acc, train loss, val acc, val loss\n",
    "\n",
    "\t\tbest_performance_df = pd.DataFrame({'Train Loss': 1000, 'Val Loss': 1000, 'Train Acc': 0, 'Val Acc': 0}, index = [name])\n",
    "\t\tbest_val_loss = 1e9\n",
    "\t\tbest_val_acc = 0\n",
    "\n",
    "\t\tself.end_training = 0\n",
    "\t\tself.extra_trained = 0\n",
    "\n",
    "\t\tfor epoch in range(1, n_epochs+1):\n",
    "\t\t\t# monitor training loss\n",
    "\t\t\ttrain_loss = 0.0\n",
    "\t\t\ttrain_acc_count = 0\n",
    "\t\t\ttotal_sample = 0\n",
    "\t\t\t\n",
    "\t\t\t###################\n",
    "\t\t\t# train the model #\n",
    "\t\t\t###################\n",
    "\t\t\tfor data in train_loader:\n",
    "\t\t\t\t# _ stands in for labels, here\n",
    "\t\t\t\t# no need to flatten images\n",
    "\t\t\t\tinputs, labels = data\n",
    "\n",
    "\t\t\t\tperform_aug = np.random.random() < 0.4\n",
    "\t\t\t\tif augment != None and perform_aug:\n",
    "\t\t\t\t\tbatch, perm, lmbd = augment['fn'](inputs.detach().cpu(), *augment['args'], **augment['kwargs'])\n",
    "\t\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t\tinputs = batch.cuda()\n",
    "\n",
    "\t\t\t\t\toutputs = model(inputs)\n",
    "\t\t\t\t\tloss = criterion(outputs, labels) * lmbd + criterion(outputs, labels[perm]) * (1 - lmbd)\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tinputs = inputs.cuda()\n",
    "\t\t\t\t\tlabels = labels.cuda()\n",
    "\t\t\t\t\t# clear the gradients of all optimized variables\n",
    "\t\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\t# forward pass: compute predicted outputs by passing inputs to the model\n",
    "\t\t\t\t\toutputs = model(inputs)\n",
    "\t\t\t\t\toutputs = sm(outputs)\n",
    "\t\t\t\t\t# calculate the loss\n",
    "\t\t\t\t\tloss = criterion(outputs, labels)\n",
    "\t\t\t\t\t# backward pass: compute gradient of the loss with respect to model parameters\n",
    "\t\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\t\t# perform a single optimization step (parameter update)\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\t# update running training loss\n",
    "\t\t\t\ttrain_loss += loss.item()\n",
    "\n",
    "\t\t\t\tif augment != None and perform_aug:\n",
    "\t\t\t\t\tnew_labels = labels * lmbd + labels[perm] * (1 - lmbd)\n",
    "\t\t\t\t\ttrain_acc_count += accurate_count(outputs, new_labels)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttrain_acc_count += accurate_count(outputs, labels)\n",
    "\t\t\t\ttotal_sample += len(labels)\n",
    "\n",
    "\t\t\tif scheduler_pass != None:\n",
    "\t\t\t\tscheduler.step()\n",
    "\t\t\t\t\t\n",
    "\t\t\t# print avg training statistics \n",
    "\t\t\ttrain_loss = train_loss/len(train_loader)\n",
    "\t\t\ttrain_acc = train_acc_count / total_sample\n",
    "\n",
    "\t\t\tval_loss = 0.0\n",
    "\t\t\tval_acc_count = 0\n",
    "\t\t\ttotal_sample = 0\n",
    "\t\t\tfor i, data in enumerate(val_loader):\n",
    "\t\t\t\tinputs, labels = data                             \n",
    "\t\t\t\tinputs = inputs.to(device)\n",
    "\t\t\t\tlabels = labels.to(device)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\toutputs = model(inputs)\n",
    "\t\t\t\toutputs = sm(outputs)\n",
    "\t\t\t\tloss = criterion(outputs, labels)\n",
    "\n",
    "\t\t\t\tval_loss += loss.item()\n",
    "\t\t\t\tval_acc_count += accurate_count(outputs, labels)\n",
    "\t\t\t\ttotal_sample += len(labels)\n",
    "\n",
    "\t\t\tval_loss = val_loss / len(val_loader)\n",
    "\t\t\tval_acc = val_acc_count / total_sample\n",
    "\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f} \\tVal Loss: {val_loss:.5f} \\tTrain Acc: {train_acc:.3f} \\tVal Acc: {val_acc:.3f}\")\n",
    "\n",
    "\t\t\ttrack_loss[epoch - 1][0] = train_acc\n",
    "\t\t\ttrack_loss[epoch - 1][1] = train_loss\n",
    "\t\t\ttrack_loss[epoch - 1][2] = val_acc\n",
    "\t\t\ttrack_loss[epoch - 1][3] = val_loss\n",
    "\t\t\tdone = epoch\n",
    "\n",
    "\t\t\tif val_loss < best_val_loss:\n",
    "\t\t\t\tbest_performance_df = pd.DataFrame({'Train Loss': train_loss, 'Val Loss': val_loss, 'Train Acc': train_acc, 'Val Acc': val_acc}, index = [name])\n",
    "\t\t\t\tself.end_training = epoch\n",
    "\t\t\t\tbest_val_loss = val_loss\n",
    "\t\t\t\tif save_model:\n",
    "\t\t\t\t\tself.save_model(name, model)\n",
    "\t\t\telif val_acc > best_val_acc:\n",
    "\t\t\t\tbest_performance_df = pd.DataFrame({'Train Loss': train_loss, 'Val Loss': val_loss, 'Train Acc': train_acc, 'Val Acc': val_acc}, index = [name])\n",
    "\t\t\t\tself.end_training = epoch\n",
    "\t\t\t\tbest_val_acc = val_acc\n",
    "\t\t\t\tif save_model:\n",
    "\t\t\t\t\tself.save_model(name, model)\n",
    "\n",
    "\t\t\tearly_stopping(val_loss, val_acc)\n",
    "\n",
    "\t\t\tif early_stopping != None and early_stopping.stop:\n",
    "\t\t\t\tif (verbose):\n",
    "\t\t\t\t\tprint(f\"Stopping due to early stopping | patience = {early_stopping.patience}\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\t\ttrack_loss.resize((done, 4))\n",
    "\t\tself.record(name, track_loss, best_performance_df)\n",
    "\n",
    "\t\tif return_model:\n",
    "\t\t\tdel train_loader, val_loader, inputs, labels\n",
    "\t\t\ttorch.cuda.empty_cache()\n",
    "\t\t\treturn model\n",
    "\t\telse:\n",
    "\t\t\tdel model, optimizer, train_loader, val_loader, inputs, labels\n",
    "\t\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\tdef record(self, name, track_loss, best_performance_df):\n",
    "\t\ttrack_loss_df = pd.DataFrame(track_loss, columns=['Train Acc', 'Train Loss', 'Val Acc', 'Val Loss'])\n",
    "\t\ttrack_loss_df.index += 1\n",
    "\t\ttrack_loss_df.index.name = 'Epoch'\n",
    "\n",
    "\t\tself.history = pd.concat([self.history, best_performance_df], axis=0)\n",
    "\t\tself.loss_tracker[name] = track_loss_df\n",
    "\n",
    "\tdef save_model(self, name, model):\n",
    "\t\tself.saves[name] = copy.deepcopy(model)\n",
    "\n",
    "\tdef get_model(self, name):\n",
    "\t\treturn copy.deepcopy(self.saves[name])\n",
    "\n",
    "\tdef archive(self, name):\n",
    "\t\tfig = plt.figure(figsize=(14, 5))\n",
    "\t\ttrain, val = sns.color_palette('Set2')[0:2]\n",
    "\t\tloss, acc = fig.subplots(1, 2)\n",
    "\n",
    "\t\tloss.axvspan(1, self.end_training, color=sns.color_palette('Paired')[0], alpha=0.4, lw=0, label='Training') \n",
    "\n",
    "\t\tloss.plot(self.loss_tracker[name].index, self.loss_tracker[name]['Train Loss'], label='Train', color=train)\n",
    "\t\tloss.plot(self.loss_tracker[name].index, self.loss_tracker[name]['Val Loss'], label='Val', color=val)\n",
    "\t\tloss.set_xlabel('Epoch')\n",
    "\t\tloss.set_ylabel('Loss')\n",
    "\n",
    "\t\tacc.axvspan(1, self.end_training, color=sns.color_palette('Paired')[0], alpha=0.4, lw=0, label='Training') \n",
    "\t\tacc.plot(self.loss_tracker[name].index, self.loss_tracker[name]['Train Acc'], label='Train', color=train)\n",
    "\t\tacc.plot(self.loss_tracker[name].index, self.loss_tracker[name]['Val Acc'], label='Val', color=val)\n",
    "\n",
    "\t\tacc.set_xlabel('Epoch')\n",
    "\t\tacc.set_ylabel('Accuracy')\n",
    "\t\tacc.set_ylim(0.60, 1)\n",
    "\n",
    "\n",
    "\t\tacc.legend()\n",
    "\t\tloss.legend()\n",
    "\t\t\n",
    "\t\tfig.savefig(f'plots/{name}.png')\n",
    "\t\tplt.close()\n",
    "\t\tdel self.loss_tracker[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our trainer with our validation data, along with our loss function: <strong>categorical crossentropy</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures\n",
    "We can now get started with determining the best non-fine-tuned model to proceed with.\n",
    "\n",
    "#### Baseline - NN\n",
    "As our baseline, we'll use a simple neural network consisting of:\n",
    "<ol>\n",
    "<li>Linear layer</li>\n",
    "<li>Batch Normalization</li>\n",
    "<li>ReLU</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "    class BaseNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(BaseNN, self).__init__()\n",
    "\n",
    "            self.flat = nn.Flatten(1)\n",
    "\n",
    "            self.fc1 = nn.Linear(784, 256)\n",
    "            self.fc1_bn = nn.BatchNorm1d(256)\n",
    "            self.fc1_act = nn.ReLU()\n",
    "\n",
    "            self.fc2 = nn.Linear(256, 64)\n",
    "            self.fc2_bn = nn.BatchNorm1d(64)\n",
    "            self.fc2_act = nn.ReLU()\n",
    "\n",
    "            self.fc3 = nn.Linear(64, 16)\n",
    "            self.fc3_bn = nn.BatchNorm1d(16)\n",
    "            self.fc3_act = nn.ReLU()\n",
    "\n",
    "            self.fc4 = nn.Linear(16, 10)\n",
    "            self.fc4_bn = nn.BatchNorm1d(10)\n",
    "            self.fc4_act = nn.ReLU()\n",
    "\n",
    "        def get_logits(self, x):\n",
    "            x = self.flat(x)\n",
    "            x = self.fc1_act(self.fc1_bn(self.fc1(x)))\n",
    "            x = self.fc2_act(self.fc2_bn(self.fc2(x)))\n",
    "            x = self.fc3_act(self.fc3_bn(self.fc3(x)))\n",
    "            x = self.fc4_act(self.fc4_bn(self.fc4(x)))\n",
    "            logits = x\n",
    "            return logits\n",
    "\n",
    "        def forward(self, x):\n",
    "            logits = self.get_logits(x)\n",
    "            return nn.functional.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'Baseline NN', BaseNN, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=True\n",
    "\t)\n",
    "\ttrainer.archive('Baseline NN')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/Baseline NN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ul>\n",
    "\t<li>Despite using early stopping, we can identify <strong>large amounts of overfitting</strong></li>\n",
    "\t<li>This suggests not that early stopping is bad, but using this model means we allow the model to overfit, <strong>as long as the validation is improving.</strong></li>\n",
    "\t<li>This means that training is perhaps inefficient but not terrible, as for each epoch, <strong>we learn more noise than pattern</strong></li>\n",
    "\t<li>With a patience parameter of <code>10</code>, Early Stopping has stopped our training at <strong>Epoch = 20</strong></li>\n",
    "\t<li>Surprisingly, the baseline performance is impressive for a basic model at 0.89 accuracy</strong>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation + Baseline NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'Baseline NN + Aug_Erase', BaseNN, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5}, # optimizer\n",
    "\t\ttrain_augmented_erase, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=True\n",
    "\t)\n",
    "\ttrainer.archive('Baseline NN + Aug_Erase')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks (LeNet)\n",
    "\n",
    "Convolutional Neural Networks (also known as ConvNets or CNNs) is a class of neural networks, with its most popular use case being in analyzing visual imagery (Valueva et al., 2020).\n",
    "\n",
    "We'll use the LNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\tclass LeNet(nn.Module):\n",
    "\t\tdef __init__(self):\n",
    "\t\t\tsuper(LeNet, self).__init__()\n",
    "\n",
    "\t\t\tself.conv1 = nn.Conv2d(1, 6, 5, padding = 2)\n",
    "\t\t\tself.sig1 = nn.Sigmoid()\n",
    "\t\t\tself.pool1 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "\t\t\tself.conv2 = nn.Conv2d(6, 16, 5, padding = 0)\n",
    "\t\t\tself.sig2 = nn.Sigmoid()\n",
    "\t\t\tself.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "\t\t\tself.flat = nn.Flatten(start_dim=1)\n",
    "\n",
    "\t\t\tself.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\t\t\tself.sig_fc1 = nn.Sigmoid()\n",
    "\n",
    "\t\t\tself.fc2 = nn.Linear(120, 84)\n",
    "\t\t\tself.sig_fc2 = nn.Sigmoid()\n",
    "\n",
    "\t\t\tself.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx = self.pool1(self.sig1(self.conv1(x)))\n",
    "\t\t\tx = self.pool2(self.sig2(self.conv2(x)))\n",
    "\t\t\tx = self.flat(x)\n",
    "\t\t\tx = self.sig_fc1(self.fc1(x))\n",
    "\t\t\tx = self.sig_fc2(self.fc2(x))\n",
    "\t\t\tx = self.fc3(x)\n",
    "\t\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'LeNet', LeNet, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('LeNet')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation + CNN (LeNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'LeNet + Aug_Erase', LeNet, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_augmented_erase, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('LeNet + Aug_Erase')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\tdef relu_bn_conv(in_channels, out_channels, **kwargs):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels, out_channels, **kwargs),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\n",
    "\tclass ResBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, skip = False):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.skip = nn.Sequential()\n",
    "\t\t\tself.conv1 = relu_bn_conv(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "\n",
    "\t\t\tif skip:\n",
    "\t\t\t\tself.conv1 = relu_bn_conv(in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1)\n",
    "\t\t\t\tself.skip = nn.Conv2d(in_channels, out_channels, kernel_size= 1, stride = 2)\n",
    "\n",
    "\t\t\tself.conv2 = relu_bn_conv(out_channels, out_channels, kernel_size = 3, padding = 1)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tskip = self.skip(x)\n",
    "\t\t\tx = self.conv1(x)\n",
    "\t\t\tx = self.conv2(x)\n",
    "\t\t\tx = x + skip\n",
    "\t\t\treturn nn.ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\tclass ResNetS(nn.Module):\n",
    "\t\tdef __init__(self, base_filter = 16):\n",
    "\t\t\tsuper().__init__()\n",
    "\n",
    "\t\t\tin_channels = 1\n",
    "\t\t\t\n",
    "\t\t\tself.l0 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_channels, base_filter, kernel_size = 7, stride = 2, padding = 3),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),\n",
    "\t\t\t\tnn.BatchNorm2d(base_filter),\n",
    "\t\t\t\tnn.ReLU()\n",
    "\t\t\t)\n",
    "\t\t\tself.l1 = nn.Sequential(\n",
    "\t\t\t\tResBlock(base_filter, base_filter, skip = False),\n",
    "\t\t\t\tResBlock(base_filter, base_filter, skip = False),\n",
    "\t\t\t)\n",
    "\t\t\tself.l2 = nn.Sequential(\n",
    "\t\t\t\tResBlock(base_filter, base_filter * 2, skip = True),\n",
    "\t\t\t\tResBlock(base_filter * 2, base_filter * 2, skip = False),\n",
    "\t\t\t)\n",
    "\t\t\tself.l3 = nn.Sequential(\n",
    "\t\t\t\tResBlock(base_filter * 2, base_filter * 3, skip = True),\n",
    "\t\t\t\tResBlock(base_filter * 3, base_filter * 3, skip = False),\n",
    "\t\t\t)\n",
    "\t\t\tself.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
    "\t\t\tself.fc = nn.Sequential(\n",
    "\t\t\t\tnn.Flatten(start_dim=1),\n",
    "\t\t\t\ttorch.nn.Linear(base_filter * 3, 10)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx = self.l0(x)\n",
    "\t\t\tx = self.l1(x)\n",
    "\t\t\tx = self.l2(x)\n",
    "\t\t\tx = self.l3(x)\n",
    "\t\t\tx = self.gap(x)\n",
    "\t\t\tx = self.fc(x)\n",
    "\n",
    "\t\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'ResNet-S', ResNetS, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('ResNet-S')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation + ResNet-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'ResNet-S + Aug_Erase', ResNetS, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_augmented_erase, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('ResNet-S + Aug_Erase')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\tclass ResNet18(nn.Module):\n",
    "\t\tdef __init__(self, base_filter = 64):\n",
    "\t\t\tsuper().__init__()\n",
    "\n",
    "\t\t\tin_channels = 1\n",
    "\t\t\t\n",
    "\t\t\tself.l0 = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_channels, base_filter, kernel_size = 7, stride = 2, padding = 3),\n",
    "\t\t\t\tnn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),\n",
    "\t\t\t\tnn.BatchNorm2d(base_filter),\n",
    "\t\t\t\tnn.ReLU()\n",
    "\t\t\t)\n",
    "\t\t\tself.l1 = nn.Sequential(\n",
    "\t\t\t\tResBlock(base_filter, base_filter, skip = False),\n",
    "\t\t\t\tResBlock(base_filter, base_filter, skip = False),\n",
    "\t\t\t)\n",
    "\t\t\tself.l2 = nn.Sequential(\n",
    "\t\t\t\tResBlock(base_filter, base_filter * 2, skip = True),\n",
    "\t\t\t\tResBlock(base_filter * 2, base_filter * 2, skip = False),\n",
    "\t\t\t)\n",
    "\t\t\tself.l3 = nn.Sequential(\n",
    "\t\t\t\tResBlock(base_filter * 2, base_filter * 3, skip = True),\n",
    "\t\t\t\tResBlock(base_filter * 3, base_filter * 3, skip = False),\n",
    "\t\t\t)\n",
    "\t\t\tself.l4 = nn.Sequential(\n",
    "\t\t\t\tResBlock(base_filter * 3, base_filter * 4, skip = True),\n",
    "\t\t\t\tResBlock(base_filter * 4, base_filter * 4, skip = False),\n",
    "\t\t\t)\n",
    "\t\t\tself.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
    "\t\t\tself.fc = nn.Sequential(\n",
    "\t\t\t\tnn.Flatten(start_dim=1),\n",
    "\t\t\t\ttorch.nn.Linear(base_filter * 4, 10)\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx = self.l0(x)\n",
    "\t\t\tx = self.l1(x)\n",
    "\t\t\tx = self.l2(x)\n",
    "\t\t\tx = self.l3(x)\n",
    "\t\t\tx = self.l4(x)\n",
    "\t\t\tx = self.gap(x)\n",
    "\t\t\tx = self.fc(x)\n",
    "\n",
    "\t\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'ResNet18', ResNet18, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('ResNet18')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation + ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'ResNet18 + Aug_Erase', ResNet18, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_augmented_erase, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False,\n",
    "\t\tbatch_size=256\n",
    "\t)\n",
    "\ttrainer.archive('ResNet18 + Aug_Erase')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGBlock(in_channels, out_channels, pool = False, activation = nn.ReLU):\n",
    "\tif pool:\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tactivation(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t)\n",
    "\treturn nn.Sequential(\n",
    "\t\tnn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\tactivation()\n",
    "\t)\n",
    "\n",
    "class VGG13(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.l0 = VGGBlock(1, 64)\n",
    "\t\tself.l1 = VGGBlock(64, 64, pool = True)\n",
    "\t\tself.l2 = VGGBlock(64, 128)\n",
    "\t\tself.l3 = VGGBlock(128, 128, pool = True)\n",
    "\t\tself.l4 = VGGBlock(128, 256)\n",
    "\t\tself.l5 = VGGBlock(256, 256)\n",
    "\t\tself.l6 = VGGBlock(256, 256, pool = True)\n",
    "\t\tself.l7 = VGGBlock(256, 512)\n",
    "\t\tself.l8 = VGGBlock(512, 512)\n",
    "\t\tself.l9 = VGGBlock(512, 512, pool = True)\n",
    "\t\tself.fc0 = nn.Sequential(\n",
    "\t\t\tnn.Flatten(start_dim = 1),\n",
    "\t\t\tnn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(512, 4096),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\t\tself.fc1 = nn.Sequential(\n",
    "\t\t\tnn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(4096, 4096),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "\t\tself.fc2 = nn.Sequential(\n",
    "\t\t\tnn.Linear(4096, 10)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.l0(x)\n",
    "\t\tx = self.l1(x)\n",
    "\t\tx = self.l2(x)\n",
    "\t\tx = self.l3(x)\n",
    "\t\tx = self.l4(x)\n",
    "\t\tx = self.l5(x)\n",
    "\t\tx = self.l6(x)\n",
    "\t\tx = self.l7(x)\n",
    "\t\tx = self.l8(x)\n",
    "\t\tx = self.l9(x)\n",
    "\t\tx = self.fc0(x)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "    \n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"VGG13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'VGG13', VGG13, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('VGG13')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Augmentation + VGG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'VGG13 + Aug_Erase', VGG13, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_augmented_erase, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('VGG13 + Aug_Erase')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "\tDeep convolution structure performs better than residual networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified CoAtNet-0\n",
    "With the rise in transformers, there has been a growing interest in the <strong>Attention mechanism</strong> and its uses in computer vision.\n",
    "\n",
    "The problem is inductive bias of vision transformers, without being pretrained on a dataset, the performance is found to be worse than state of the art convnet. (https://analyticsindiamag.com/a-guide-to-coatnet-the-combination-of-convolution-and-attention-networks/). \n",
    "\n",
    "Below, we use CoAtNet-0, which is an architecture that <strong>marries</strong> Convolution blocks (specifically MBConvs from MobileNet) and Transformer Blocks\"\n",
    "\n",
    "However, the smallest model CoAtNet-0 is quite an overkill for this dataset due to it's large number of parameters, despite being the smallest version. As such, we the following adjustments to make the model smaller.\n",
    "\n",
    "<ul>\n",
    "\t<li>Change from the architecture of\n",
    "\t<code>Conv + Conv + Trans + Trans -> Conv + Conv + Conv + Trans </code>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<strong>Reduced</strong> downsampling frequency to downsample only at layer 3 and 5. This is such due <strong>to the smaller image size</strong> of our data being <code>28x28</code>.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\t<strong>Downsampling frequency skewed to the right</strong>. This means we move the downsampling layers towards the later, such that we can <strong>increase the amount of information that the earlier layers can capture.</strong>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tThe <strong>number of blocks and channels were also reduced</strong>. This is done as the dataset is quite small and does not need an incredibly deep network (CoAtNet-0 was originally meant for <code>224x224x3</code> images)\n",
    "\t</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample == False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample == False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "\n",
    "        self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "        self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                # down-sample in the first conv\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
    "                          groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else:\n",
    "            return self.proj(x) + self.conv(x)\n",
    "\n",
    "# torch.nn.MultiheadAttention()\n",
    "# self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "        self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "        self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else:\n",
    "            x = self.proj(x) + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_size = (28, 28)\n",
    "        in_channels = 1\n",
    "        num_blocks = [2, 2, 3, 3, 2]            # L\n",
    "        channels = [64, 64, 128, 128, 256]\n",
    "        num_classes = 10\n",
    "        block_types = ['C', 'C', 'C', 'T']\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': Transformer}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 1, iw // 1), downsample = False)\n",
    "        self.s1 = self._make_layer(\n",
    "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 1, iw // 1), downsample = False)\n",
    "        self.s2 = self._make_layer(\n",
    "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 2, iw // 2), downsample = True)\n",
    "        self.s3 = self._make_layer(\n",
    "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 2, iw // 2), downsample = False)\n",
    "        self.s4 = self._make_layer(\n",
    "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 4, iw // 4), downsample = True)\n",
    "        self.pool = nn.AvgPool2d(ih // 4, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        global error_maker\n",
    "        # if x.shape[1] == 1 and x.shape[2] == 28:\n",
    "        #     error_maker = copy.deepcopy(x)\n",
    "        x = self.s0(x)\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size, downsample = False):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=downsample))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Modified CoAtNet0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'Modified CoAtNet0', CoAtNet, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('Modified CoAtNet0')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Augmentation + CoAtNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'Modified CoAtNet0 + Aug_Erase', CoAtNet, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_augmented_erase, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('Modified CoAtNet0 + Aug_Erase')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that CoAtNet appears to work well. However, let's investigate whether or not this is <strong>due to the Transformer blocks, or due to the MBConv layer.</strong>\n",
    "\n",
    "To find this out, we'll test the model architecture that first introduced the MBConv layers, MobileNet. This experiment is not expensive, as MobileNet has beeen designed for efficiency purposes.\n",
    "\n",
    "#### Modified MobileNetV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv0 = nn.Conv2d(1, 32, 1)\n",
    "\t\tself.mb1 = MBConv(32, 16, (28, 28), downsample = False, expansion = 1)\n",
    "\t\tself.mb2 = MBConv(16, 24, (28, 28), downsample = False, expansion = 6)\n",
    "\t\tself.mb3 = MBConv(24, 32, (14, 14), downsample = True, expansion = 6)\n",
    "\t\tself.mb4 = MBConv(32, 64, (14, 14), downsample = False, expansion = 6)\n",
    "\t\tself.mb5 = MBConv(64, 96, (7, 7), downsample = True, expansion = 6)\n",
    "\t\tself.mb6 = MBConv(96, 160, (7, 7), downsample = False, expansion = 6)\n",
    "\t\tself.mb7 = MBConv(160, 320, (3, 3), downsample = True, expansion = 6)\n",
    "\t\tself.conv1 = nn.Conv2d(320, 1280, 1)\n",
    "\t\tself.conv2 = nn.AvgPool2d(3)\n",
    "\t\tself.conv3 = nn.Conv2d(1280, 10, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv0(x)\n",
    "\t\tx = self.mb1(x)\n",
    "\t\tx = self.mb2(x)\n",
    "\t\tx = self.mb3(x)\n",
    "\t\tx = self.mb4(x)\n",
    "\t\tx = self.mb5(x)\n",
    "\t\tx = self.mb6(x)\n",
    "\t\tx = self.mb7(x)\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\tx = self.conv3(x)\n",
    "\t\tx = x[:,:,0,0]\n",
    "\t\treturn x\n",
    "\n",
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'MobileNetV2', MobileNetV2, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False\n",
    "\t)\n",
    "\ttrainer.archive('MobileNetV2')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation + Modified MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_DEBUG:\n",
    "\ttrainer.train_model(\n",
    "\t\t'MobileNetV2 + Aug_Erase', MobileNetV2, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\ttrain_data, # data\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=True\n",
    "\t)\n",
    "\ttrainer.archive('MobileNetV2 + Aug_Erase')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that both VGG13 and Modified CoAtNet-0 with <strong>no data augmentation</strong> lead to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model size vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Augmentation Technique\n",
    "We recognize that although data augmentation is supposed to improve our scores, it appears that our data augmentation does not have a positive effect. As such, we can try two different types of augmentation:\n",
    "<ul>\n",
    "\t<li>CutMix</li>\n",
    "\t<li>FMix</li>\n",
    "</ul>\n",
    "\n",
    "CutMix essentially takes one rectangle patch from an image, and replaces this patch with another image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(inputs, beta):\n",
    "    lam = np.random.beta(beta, beta)\n",
    "    rand_index = torch.randperm(inputs.size()[0])\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "    inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "    # new_labels = labels * lam + labels[rand_index] * (1 - lam)\n",
    "    return inputs, rand_index, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not MODEL_IMPROVE_DEBUG:\n",
    "\ttrainer = Trainer(nn.CrossEntropyLoss)\n",
    "\n",
    "\ttrainer.train_model(\n",
    "\t\t'VGG13 + Aug_CutMix', VGG13, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\t# train_cutmix_base_data, # data\n",
    "\t\ttrain_data,\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=True,\n",
    "\t\taugment={\n",
    "\t\t\t'fn': cutmix,\n",
    "\t\t\t'args': [1],\n",
    "\t\t\t'kwargs': {}\n",
    "\t\t}\n",
    "\t)\n",
    "\n",
    "\ttrainer.archive('VGG13 + Aug_CutMix')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_IMPROVE_DEBUG:\n",
    "\ttrainer = Trainer(nn.CrossEntropyLoss)\n",
    "\n",
    "\ttrainer.train_model(\n",
    "\t\t'Modified CoAtNet0 + Aug_CutMix', CoAtNet, # model\n",
    "\t\toptim.Adam, {'lr': 1e-3 * (GPU_COUNT) ** 0.5, 'weight_decay': 1e-5}, # optimizer\n",
    "\t\t# train_cutmix_base_data, # data\n",
    "\t\ttrain_data,\n",
    "\t\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\t\tverbose=False,\n",
    "\t\taugment={\n",
    "\t\t\t'fn': cutmix,\n",
    "\t\t\t'args': [1],\n",
    "\t\t\t'kwargs': {}\n",
    "\t\t}\n",
    "\t)\n",
    "\n",
    "\ttrainer.archive('Modified CoAtNet0 + Aug_CutMix')\n",
    "\tdisplay(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that CutMix actually improves our model performance, to an <strong>all new best validation accuracy of 0.94</strong>. Let's continue using CutMix for data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning Optimizer\n",
    "\n",
    "Next, we can perform tuning to the optimizer and the training.\n",
    "\n",
    "#### Optimizer Tuning\n",
    "We'll try both <strong>AdamW</strong> and <strong>SGD</strong>, in addition to tuning the learning rate.\n",
    "<strong>SGD</strong> likely will take more epochs than Adam and <strong>will have more drastic and random</strong> results, however this behaviour will often lead to a more robust model.\n",
    "<strong>AdamW</strong> is similar to Adam, however the weight decay (l2 regularization) term is now separated from the gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "param_grid = {\n",
    "\t'optim': [optim.SGD, optim.AdamW],\n",
    "\t'lr': [1e-3, 1e-4],\n",
    "\t'weight_decay': [1e-4, 1e-5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduling\n",
    "We'll also add in a learning rate scheduler. Learning rate schedulers adjust the learning rate over the epochs the model trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid['scheduler'] = [\n",
    "\t[optim.lr_scheduler.CosineAnnealingLR, {'T_max': 5}],\n",
    "\t[optim.lr_scheduler.StepLR, {\"step_size\": 10, \"gamma\": 0.1}],\n",
    "\t[optim.lr_scheduler.StepLR, {\"step_size\": 20, \"gamma\": 0.1}],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture change\n",
    "\n",
    "Lastly, we try to <strong>replace the ReLU activation functions</strong> in VGG13 with <strong>GELU</strong>, as it is not as affected by the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGBlock(in_channels, out_channels, pool = False, activation = nn.GELU):\n",
    "\tif pool:\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "\t\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\t\tactivation(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\t\t)\n",
    "\treturn nn.Sequential(\n",
    "\t\tnn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "\t\tnn.BatchNorm2d(out_channels),\n",
    "\t\tactivation()\n",
    "\t)\n",
    "\n",
    "class GELU_VGG13(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.l0 = VGGBlock(1, 64)\n",
    "\t\tself.l1 = VGGBlock(64, 64, pool = True)\n",
    "\t\tself.l2 = VGGBlock(64, 128)\n",
    "\t\tself.l3 = VGGBlock(128, 128, pool = True)\n",
    "\t\tself.l4 = VGGBlock(128, 256)\n",
    "\t\tself.l5 = VGGBlock(256, 256)\n",
    "\t\tself.l6 = VGGBlock(256, 256, pool = True)\n",
    "\t\tself.l7 = VGGBlock(256, 512)\n",
    "\t\tself.l8 = VGGBlock(512, 512)\n",
    "\t\tself.l9 = VGGBlock(512, 512, pool = True)\n",
    "\t\tself.fc0 = nn.Sequential(\n",
    "\t\t\tnn.Flatten(start_dim = 1),\n",
    "\t\t\tnn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(512, 4096),\n",
    "\t\t\tnn.GELU()\n",
    "\t\t)\n",
    "\t\tself.fc1 = nn.Sequential(\n",
    "\t\t\tnn.Dropout(0.5),\n",
    "\t\t\tnn.Linear(4096, 4096),\n",
    "\t\t\tnn.GELU()\n",
    "\t\t)\n",
    "\t\tself.fc2 = nn.Sequential(\n",
    "\t\t\tnn.Linear(4096, 10)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.l0(x)\n",
    "\t\tx = self.l1(x)\n",
    "\t\tx = self.l2(x)\n",
    "\t\tx = self.l3(x)\n",
    "\t\tx = self.l4(x)\n",
    "\t\tx = self.l5(x)\n",
    "\t\tx = self.l6(x)\n",
    "\t\tx = self.l7(x)\n",
    "\t\tx = self.l8(x)\n",
    "\t\tx = self.l9(x)\n",
    "\t\tx = self.fc0(x)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"GELU_VGG13\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding parameter for models\n",
    "We'll add a parameter of <code>model</code> in our parameter search space. This is because these models have fairly similar scores, and so it is worth trying to hyperparameter tune all 3 to see which model we can get the best scores from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid['model'] = [GELU_VGG13, VGG13, CoAtNet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done defining our parameter search grid, we can now proceed with running the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_IMPROVE_DEBUG:\n",
    "    param_list = list(ParameterSampler(param_grid, n_iter = 10))\n",
    "\n",
    "    iter_list = [dict((k, v) for (k, v) in d.items()) for d in param_list]\n",
    "\n",
    "    for i, params in enumerate(iter_list):\n",
    "        print(f'Params {i}: ', params)\n",
    "\n",
    "        trainer.train_model(\n",
    "            f'{params[\"model\"]()} + Aug_CutMix + FineTuned v{i}', params[\"model\"], # model\n",
    "            params['optim'], {'lr': params['lr'] * (GPU_COUNT) ** 0.5, 'weight_decay': params['weight_decay']}, # optimizer\n",
    "            # train_augmented_base_data, # data\n",
    "            train_data,\n",
    "            # train_augmented_erase,\n",
    "            early_stopping = CustomEarlyStopping(patience = 10),\n",
    "            scheduler_pass = params['scheduler'][0],\n",
    "            scheduler_args = params['scheduler'][1],\n",
    "            verbose=False,\n",
    "            augment = {\n",
    "                'fn': cutmix,\n",
    "                'args': [1],\n",
    "                'kwargs': {}\n",
    "            }\n",
    "        )\n",
    "\n",
    "        trainer.archive(f'{params[\"model\"]()} + Aug_CutMix + FineTuned v{i}')\n",
    "    display(trainer.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our fine tune stage, our best model is  and has a validation accuracy of .\n",
    "Due to insufficient memory space, we had to delete our models during the searching period. As such we <strong>need to extract the architecture</strong> and <strong>hyperparameters</strong>, and <strong>retrain the model.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = trainer.train_model(\n",
    "\t'ModifiedCoAtNet0 + Aug_CutMix + FineTuned Final', CoAtNet, # model\n",
    "\toptim.AdamW, {'lr': 1e-3 * (GPU_COUNT) ** 0.5}, # optimizer\n",
    "\ttrain_data, # data\n",
    "\tearly_stopping = CustomEarlyStopping(patience = 15),\n",
    "\tverbose = True,\n",
    "\tscheduler_pass = optim.lr_scheduler.CosineAnnealingLR,\n",
    "\tscheduler_args = {'T_max': 5},\n",
    "\treturn_model = True,\n",
    "\taugment = {\n",
    "\t\t'fn': cutmix,\n",
    "\t\t'args': [1],\n",
    "\t\t'kwargs': {}\n",
    "\t}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.archive('ModifiedCoAtNet0 + Aug_CutMix + FineTuned Final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loading the test dataset.\n",
    "test = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "test[test.columns[test.columns != 'label']] = preprocess(test[test.columns[test.columns != 'label']])\n",
    "test_data = TorchDataset(test.drop('label', axis=1), test['label'])\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "# Performing inference\n",
    "test_acc_count = 0\n",
    "total_sample = 0\n",
    "\n",
    "output = []\n",
    "label = []\n",
    "\n",
    "sm = nn.Softmax(dim=1)\n",
    "for i, data in enumerate(test_loader):\n",
    "\tinputs, labels = data                             \n",
    "\tinputs = inputs.to(device)\n",
    "\tlabels = labels.to(device)\n",
    "\n",
    "\toutputs = final_model(inputs)\n",
    "\toutputs = sm(outputs)\n",
    "    \n",
    "\tlabel.append(labels.detach().cpu().numpy())\n",
    "\toutput.append(outputs.detach().cpu().numpy())\n",
    "\ttest_acc_count += accurate_count(outputs, labels)\n",
    "\ttotal_sample += len(labels)\n",
    "\n",
    "\ttest_acc = test_acc_count / total_sample\n",
    "\n",
    "\n",
    "print(\"Final Model | Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.concatenate(output, axis=0)\n",
    "label = np.concatenate(label, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-wise accuracy + distribution of errors\n",
    "Let's perform an error analysis and see which classes our model performs the worst on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(0, 10):\n",
    "\tidx = np.array(list(map(lambda x: np.argmax(x), label))) == i\n",
    "\toutput_arr = np.array(list(map(lambda x: np.argmax(x), output[idx])))\n",
    "\tarr.append([classes[i], (output_arr == i).sum() / len(output_arr)])\n",
    "\n",
    "pd.DataFrame(arr, columns = ['Item', 'Accuracy']).sort_values(by='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model appears to make errors concerning the classes of <code>Shirt</code>, <code>T-shirt/top</code>, <code>Pullover</code> and <code>Coat</code>. As we've previously seen before during the EDA, we remember that <strong>TSNE</strong> showed us that these items had a <strong>large overlap of area</strong> in the 2d manifold representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 images with the largest error\n",
    "Let's take a look at what images have the largest error. To do this, we'll perform inference and find the images that produce the <strong>largest Cross-Entropy Loss</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "a = np.array(list(map(lambda x: loss_func(torch.tensor(x[0]), torch.tensor(x[1])).detach().cpu().numpy(), list(zip(output, label)))))\n",
    "idx = np.argpartition(a, -10)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = list(map(lambda x: classes[x], test['label'].iloc[idx].values.tolist()))\n",
    "pred_labels = list(map(lambda x: classes[np.argmax(x)], label[idx]))\n",
    "\n",
    "imshow(\n",
    "\ttest_data.x[idx][:,0].detach().cpu().numpy(),\n",
    "\t[f'True: {true}\\nPred:{pred}' for true, pred in zip(true_labels, pred_labels)]\n",
    "\t, figsize=(10, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the errors the model makes are:\n",
    "<ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We were able to produce a model that keeps up with SOTA performance scores in the benchmark, roughly placing us around the <strong>10th position</strong> in <code>paperswithcode.com</code>.\n",
    "\n",
    "I have learned a lot about computer vision models, and in particular the <strong>attention mechanism</strong>. I look forward to deploying the newly learned knowledge and techniques into my future notebooks.\n",
    "\n",
    "Author: Kenneth Chen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1042389d7c82dd1bd8119cafb1c36337ac9bb25498b1a7ccb724fef191fad074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
